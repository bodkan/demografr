--
title: "Downstream analyses, model selection, troubleshooting"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Downstream analyses, model selection, troubleshooting"}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
slendr_present <- slendr::check_dependencies(python = TRUE, slim = TRUE, quit = FALSE)

knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#>",
  fig.align = "center",
  fig.width = 8,
  fig.height = 5,
  dpi = 80,
  eval = slendr_present && FALSE
)

dataX_path <- here::here("inst/examples/downstream_dataX.rds")
dataY_path <- here::here("inst/examples/downstream_dataY.rds")
dataZ_path <- here::here("inst/examples/downstream_dataZ.rds")
data_path <- here::here("inst/examples/downstream_cv.rds")
```

```{r, collapse=TRUE}
library(demografr)
library(slendr)
init_env()

library(future)
plan(multicore, workers = availableCores())

SEED <- 42
set.seed(SEED)
```

⚠️⚠️⚠️

**Note:** The _demografr_ R package is still under active development. As a result, its documentation is in a draft stage at best. Apologies for typos, inconsistencies, and other issues.

**This vignette in particular is currently being used as a testing ground for implementing the support of various validation features of the _abc_ R package. Please do not use any of this just yet.**

⚠️⚠️⚠️

Let's return to our [first](vignette-01-basics.html) example. However, this time, imagine that we don't really know whether which of the three following phylogenetic relationships is the one that captures the features of our data best, perhaps with different sources of evidence being consistent with one of them (as always, this is purely a toy example). In other words, we want to perform *model selection*.

```{r ape_tree_modelX, echo=FALSE, fig.width=5, fig.height=5}
par(mar = c(0, 0, 2, 0), mfrow = c(3, 1))
tree <- ape::read.tree(text="(popA,(popB,(popC,popD)));")
plot(tree, main = "model X")
arrows(2.5, 2, 2.5, 3, col="blue")

tree <- ape::read.tree(text="((popA,popB),(popC,popD));")
plot(tree, main = "model Y")
arrows(2.5, 2, 2.5, 3, col="blue")

tree <- ape::read.tree(text="(((popA,popB),popC),popD);")
plot(tree, main = "model Z")
arrows(2.5, 2, 2.5, 3, col="blue")
```

For completeness, here is again our computed observed data:

1. Nucleotide diversity in each population:

```{r}
observed_diversity <- read.table(system.file("examples/observed_diversity.tsv", package = "demografr"), header = TRUE)

observed_diversity
```

2. Pairwise divergence d_X_Y between populations X and Y:

```{r}
observed_divergence <- read.table(system.file("examples/observed_divergence.tsv", package = "demografr"), header = TRUE)

observed_divergence
```

3. Value of the following $f_4$-statistic:

```{r}
observed_f4  <- read.table(system.file("examples/observed_f4.tsv", package = "demografr"), header = TRUE)

observed_f4
```

We will again bind them into a list:

```{r}
observed <- list(diversity  = observed_diversity, divergence = observed_divergence, f4 = observed_f4)
```


### Three competing models

First, in order to perform model selection, we need the models themselves. In this example, we have to options to do this.

**First**, we can define three separate functions, each of them encoding the three phylogenetic relationships:

```{r}
modelX <- function(Ne_A, Ne_B, Ne_C, Ne_D, T_1, T_2, T_3, gf) {
  popA <- population("popA", time = 1,   N = Ne_A)
  popB <- population("popB", time = T_1, N = Ne_B, parent = popA)
  popC <- population("popC", time = T_2, N = Ne_C, parent = popB)
  popD <- population("popD", time = T_3, N = Ne_D, parent = popC)

  gf <- gene_flow(from = popB, to = popC, start = 9000, end = 9301, rate = gf)

  model <- compile_model(
    populations = list(popA, popB, popC, popD), gene_flow = gf,
    generation_time = 1, simulation_length = 10000,
    direction = "forward"
  )

  return(model)
}

modelY <- function(Ne_A, Ne_B, Ne_C, Ne_D, T_1, T_2, T_3, gf) {
  popA <- population("popA", time = 1,   N = Ne_A)
  popB <- population("popB", time = T_1, N = Ne_B, parent = popA)
  popC <- population("popC", time = T_2, N = Ne_C, parent = popA)
  popD <- population("popD", time = T_3, N = Ne_D, parent = popC)

  gf <- gene_flow(from = popB, to = popC, start = 9000, end = 9301, rate = gf)

  model <- compile_model(
    populations = list(popA, popB, popC, popD), gene_flow = gf,
    generation_time = 1, simulation_length = 10000,
    direction = "forward"
  )

  return(model)
}

modelZ <- function(Ne_A, Ne_B, Ne_C, Ne_D, T_1, T_2, T_3, gf) {
  popA <- population("popA", time = 1,   N = Ne_A)
  popB <- population("popB", time = T_1, N = Ne_B, parent = popA)
  popC <- population("popC", time = T_2, N = Ne_C, parent = popA)
  popD <- population("popD", time = T_3, N = Ne_D, parent = popA)

  gf <- gene_flow(from = popB, to = popC, start = 9000, end = 9301, rate = gf)

  model <- compile_model(
    populations = list(popA, popB, popC, popD), gene_flow = gf,
    generation_time = 1, simulation_length = 10000,
    direction = "forward"
  )

  return(model)
}
```


```{r, eval=FALSE}
modelX(1, 1, 1, 1, 2000, 6000, 8000, 0.5) %>% plot_model(order = c("popA", "popB", "popC", "popD"), file = "modelX.pdf")
modelY(1, 1, 1, 1, 2000, 6000, 8000, 0.5) %>% plot_model(order = c("popA", "popB", "popC", "popD"), file = "modelY.pdf")
modelZ(1, 1, 1, 1, 2000, 6000, 8000, 0.5) %>% plot_model(order = c("popA", "popB", "popC", "popD"), file = "modelZ.pdf")
```

Now, let's [specify priors](vignette-02-priors.html) using _demografr_'s [templating syntax](vignette-02-priors.html#prior-parameter-templates) , as well as put together a list of tree-sequence summary functions and observed summary statistics:

```{r}
priors <- list(
  Ne... ~ runif(100, 10000),

  T_1   ~ runif(1,    4000),
  T_2   ~ runif(3000, 9000),
  T_3   ~ runif(5000, 10000),

  gf    ~ runif(0, 1)
)

compute_diversity <- function(ts) {
  samples <- extract_names(ts, split = "pop")
  ts_diversity(ts, sample_sets = samples)
}
compute_divergence <- function(ts) {
  samples <- extract_names(ts, split = "pop")
  ts_divergence(ts, sample_sets = samples)
}
compute_f4 <- function(ts) {
  samples <- extract_names(ts, split = "pop")
  ts_f4(ts,
        W = list(popA = samples$popA),
        X = list(popB = samples$popB),
        Y = list(popC = samples$popC),
        Z = list(popD = samples$popD))
}

functions <- list(diversity = compute_diversity, divergence = compute_divergence, f4 = compute_f4)

observed <- list(diversity = observed_diversity, divergence = observed_divergence, f4 = observed_f4)
```

```{r, eval=FALSE}
validate_abc(modelX, priors, functions, observed)
validate_abc(modelY, priors, functions, observed)
validate_abc(modelZ, priors, functions, observed)
```


```{r, eval=FALSE}
tsX <- simulate_ts(modelX, priors)
tsY <- simulate_ts(modelY, priors)
tsZ <- simulate_ts(modelZ, priors)
```


With that out of the way, we can proceed with generating simulated data for inference using all three models. What we'll do is perform three runs and save them into appropriately named variables `dataX`, `dataY`, and `dataZ`:

```{r, echo=FALSE, eval=TRUE}
tstart <- Sys.time()
```

```{r, eval=!file.exists(dataX_path)}
dataX <- simulate_abc(modelX, priors, functions, observed, iterations = 1000,
                      sequence_length = 10e6, recombination_rate = 1e-8, mutation_rate = 1e-8)
```

```{r, eval=!file.exists(dataY_path)}
dataY <- simulate_abc(modelY, priors, functions, observed, iterations = 10000,
                      sequence_length = 10e6, recombination_rate = 1e-8, mutation_rate = 1e-8)
```

```{r, eval=!file.exists(dataZ_path)}
dataZ <- simulate_abc(modelZ, priors, functions, observed, iterations = 10000,
                      sequence_length = 10e6, recombination_rate = 1e-8, mutation_rate = 1e-8)
```

```{r, echo=FALSE, eval=TRUE}
tend <- Sys.time()
tdelta <- as.numeric(difftime(tend, tstart, units = "secs"))
ncores <- future::availableCores()
```

```{r, echo=FALSE, eval=!file.exists(dataX_path)}
saveRDS(tdelta, here::here("inst/examples/downstream_tdelta.rds"))
saveRDS(ncores, here::here("inst/examples/downstream_ncores.rds"))
```

```{r, echo=FALSE, eval=file.exists(dataX_path)}
tdelta <- readRDS(here::here("inst/examples/downstream_tdelta.rds"))
ncores <- readRDS(here::here("inst/examples/downstream_ncores.rds"))
```

```{r, echo=FALSE, eval=TRUE}
hours <- floor(tdelta / 3600)
minutes <- floor((tdelta - hours * 3600) / 60)
seconds <- round(tdelta - hours * 3600 - minutes * 60)
```

**The total runtime for the ABC simulations was `r paste(hours, "hours", minutes, "minutes", seconds, "seconds")` parallelized across `r ncores` CPUs.**


```{r, echo=FALSE, eval=!file.exists(dataX_path)}
saveRDS(dataX, "dataX.rds")
```

```{r, echo=FALSE, eval=!file.exists(dataY_path)}
saveRDS(dataY, "dataY.rds")
```

```{r, echo=FALSE, eval=!file.exists(dataZ_path)}
saveRDS(dataZ, "dataZ.rds")
```

```{r, echo=FALSE, eval=file.exists(dataX_path)}
dataX <- readRDS(dataX_path)
```

```{r, echo=FALSE, eval=file.exists(dataY_path)}
dataY <- readRDS(dataY_path)
```

```{r, echo=FALSE, eval=file.exists(dataZ_path)}
dataZ <- readRDS(dataZ_path)
```

```{r, results="hide"}
abcX <- perform_abc(dataX, engine = "abc", tol = 0.01, method = "neuralnet")
abcY <- perform_abc(dataY, engine = "abc", tol = 0.01, method = "neuralnet")
abcZ <- perform_abc(dataZ, engine = "abc", tol = 0.01, method = "neuralnet")
```

## Cross-validation

Before doing model selection, it's important to perform cross-validation to answer the question whether our ABC setup can even distinguish between the competing models.

This can be done using _demografr_'s convenience interface wrapper `perform_cv()` built around _abc_'s own function `cv4postpr()`. We will not go into too much detail, as this function simply calls `cv4postpr()` under the hood, passing to it all specified function arguments. For more details, read section "Model selection" in the [vignette](https://cran.r-project.org/package=abc/vignettes/abcvignette.pdf) of the _abc_ R package.

The one difference between the two functions is that `perform_cv()` removes the need to prepare character indices and bind together summary statistic matrices from different models&mdash;given that _demografr_'s ABC output objects track all this information along in their internals, this is redundant, and you can perform cross-validation of different ABC models simply by calling this:

```{r, eval=!file.exists(data_path), results="hide", warning=FALSE}
cv_result <- perform_cv(
  models = list(modelX = abcX, modelY = abcY, modelZ = abcZ), # must be a *named* list!
  nval = 100, tol = 0.01, method = "neuralnet"
)
```

```{r, echo=FALSE, eval=!file.exists(data_path)}
saveRDS(cv_result, data_path)
```

```{r, echo=FALSE, eval=file.exists(data_path)}
cv_result <- readRDS(data_path)
```

If we print out the result, we get a quick summary with confusion matrices and other information:

```{r}
cv_result
```

Similarly, you can use the `plot()` function to visualize the result. This function, yet again, internally calls _abc_'s own plotting method internall, with a bonus option to save a figure to a PDF right from the `plot()` call (useful when working on a remote server):

```{r plot_cv}
plot(cv_result)
```

Because we have three models, each of the three barplots shows how often were summary statistics sampled from each model classified as likely coming from one of the three models. In other words, with absolutely perfect classification, each barplot would show just one of the three colors. If a barplot (results for one model) shows multiple colors, this means that some fraction of simulated statistics from that model was incorrectly classified as another model. Again, for more detail on interpretation, caveats, and best practices, please consult the _abc_ R package [vignette](https://cran.r-project.org/package=abc/vignettes/abcvignette.pdf) and a relevant statistical textbook.

The confusion matrices and the visualization all suggest that ABC can distinguish between the three models very well. For instance, _modelX_ has been classified correctly in `r cv_result$estim[[1]] %>% { .[names(.) == "modelX"] } %>% { names(.) == . } %>% sum` out of the total of 100 cross-validation simulations, with an overall misclassification rate for all models of only `r cv_result$estim[[1]] %>%  { 1 - mean(names(.) == .) } * 100`%.

## Model selection

Armed with confidence in the ability of ABC to correctly identify the right model based on simulated data, we can proceed to selection of _the best model for our empirical data set_.

```{r}
models <- list(modelX = abcX, modelY = abcY, modelZ = abcZ)

  if (is.null(names(models)) || any(names(models) == ""))
    stop("The 'models' argument must be a named list, with the names being\n",
         "unique identifier names of each model", call. = FALSE)

  if (!all(vapply(models, inherits, "demografr_abc_sims", FUN.VALUE = logical(1))) &&
      !all(vapply(models, inherits, "demografr_abc.abc", FUN.VALUE = logical(1))))
    stop("The list of models must be either all objects produced by the function\n",
         "`simulate_abc()` or function `perform_abc()` as they store full information\n",
         "about an ABC model being run, namely the simulated summary statistics", call. = FALSE)

  model_nsims <- vapply(models, function(x) nrow(attr(x, "simulated")), FUN.VALUE = integer(1))
  model_stats <- lapply(models, function(x) attr(x, "simulated")) %>% do.call(rbind, .) %>% as.matrix()
  model_names <- lapply(seq_along(models), function(i) rep(names(models)[i], model_nsims[i])) %>% unlist()

  observed_stats <- attr(abcX, "observed")

modsel <- postpr(observed_stats, model_names, model_stats, tol = 0.05, method = "mnlogistic")

summary(modsel)
```


```{r}
observed_stats <- attr(abcX, "observed")
sim_stats <- attr(abcX, "simulated")
gfitX <- gfit(target = observed_stats, sumstat = sim_stats, statistic = mean, nb.replicate = 100)

observed_stats <- attr(abcY, "observed")
sim_stats <- attr(abcY, "simulated")
gfitY <- gfit(target = observed_stats, sumstat = sim_stats, statistic = mean, nb.replicate = 100)

observed_stats <- attr(abcZ, "observed")
sim_stats <- attr(abcZ, "simulated")
gfitZ <- gfit(target = observed_stats, sumstat = sim_stats, statistic = mean, nb.replicate = 100)

summary(gfitX)
summary(gfitY)
summary(gfitZ)

plot(gfitX, main="Histogram for modelX under H0")
plot(gfitY, main="Histogram for modelY undeb H0")
plot(gfitZ, main="Histogram for modelZ under H0")
```


```{r}
  model_nsims <- vapply(models, function(x) nrow(attr(x, "simulated")), FUN.VALUE = integer(1))
  model_stats <- lapply(models, function(x) attr(x, "simulated")) %>% do.call(rbind, .) %>% as.matrix()
  model_names <- lapply(seq_along(models), function(i) rep(names(models)[i], model_nsims[i])) %>% unlist()

library(locfit)
gfitpca(target=observed_stats, sumstat=model_stats, index=model_names, cprob = 0.1)
```


```{r}
predictions <- predict(abcX, 10, stat = "diversity")
predictions
```

```{r}
plot_prediction(predictions, "diversity")

plot_prediction(predictions, "divergence")

plot_prediction(predictions, "f4")
```

If more customization or a more detailed analyses is needed, you can also extract data from the posterior simulations themselves:

```{r}
extract_prediction(predictions, "diversity")
```

However, note that `extract_prediction` is a convenience function which only unnests, reformats, and renames the list-columns with summary statistics stored in the `predictions` data frame above. You can do all the processing based on the `predictions` object above.