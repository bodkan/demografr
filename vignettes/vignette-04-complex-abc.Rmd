---
title: "Defining scaffold models"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Defining scaffold models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center"
)
```

## Introduction

Let's return to our [introductory example](vignette-01-basic-abc.html). The "scaffold" model we defined in that vignette was created using a built-in function `tree_model()`, which accepts a phylogenetic tree as input and converts it into an appropriate _slendr_ demographic model which _demografr_ later uses as a scaffold for ABC inference.

We begin by loading all required packages, and setting up parallelization for our inference using the package _future_:

```{r, message=FALSE}
library(dplyr)
library(readr)

devtools::load_all("~/Projects/slendr")
devtools::load_all()

# my Mac has 10 cores, feel free to adjust this
library(future)
plan(multisession, workers = availableCores())
```

Load observed summary statistics computed from "sequenced" real data:

```{r, message=FALSE}
map <- world(xrange = c(-90, -20), yrange = c(-58, 15))
plot_map(map)

p1 <- population("p1", time = 1000, N = 1000, map = map, radius = 3, center = c(-70, 0))
plot_map(p1)
```

Define tree-sequence-based summary functions:

```{r}
compute_diversity <- function(ts) {
  samples <- sample_names(ts, split = TRUE)
  ts_diversity(ts, sample_sets = samples) %>%
    mutate(stat = paste0("pi_", set)) %>%
    select(stat, value = diversity)
}
compute_divergence <- function(ts) {
  samples <- sample_names(ts, split = TRUE)
  ts_divergence(ts, sample_sets = samples) %>%
    mutate(stat = sprintf("d_%s_%s", x, y)) %>%
    select(stat, value = divergence)
}
functions <- list(diversity = compute_diversity, divergence = compute_divergence)
```





### Scaffold model from a tree

To recap, this is how we built that model from a phylogenetic tree:

```{r, slendr_model1, fig.width=5, fig.height=3, fig.align="center"}
model1 <- tree_model(tree = "(popA,(popB,(popC,popD)));", time_span = 10000)
plot_model(model1)
```

After specifying the priors we can plug this model into the ABC inference pipeline:

```{r}
priors <- list(
  Ne_popA ~ runif(1, 10000),
  Ne_popB ~ runif(1, 10000),
  Ne_popC ~ runif(1, 10000),
  Ne_popD ~ runif(1, 10000),

  Tsplit_popA_popB ~ runif(1, 3000),
  Tsplit_popB_popC ~ runif(3000, 6000),
  Tsplit_popC_popD ~ runif(6000, 9000)
)
```

From the previous vignette we already know this ABC model setup is valid, but it's always worth checking again:

```{r}
validate_abc(model1, priors, functions, observed)
```

In order to save computational time, we won't simulate the full ABC run. That said, for completeness, here are two commands we would usually run once the ABC setup is successfully validated. We would first simulate data for ABC inference:

```{r, eval=FALSE}
data <- simulate_abc(model1, priors, functions, observed, iterations = 10000,
                    sequence_length = 1e6, recombination_rate = 1e-8)
```

Then we would plug the simulations into the inference function `perform_abc()`:

```{r, eval=FALSE}
abc <- perform_abc(data, tolerance = 0.05, method = "neuralnet")
```








### Scaffold model from a _slendr_ model

An alternative way to define a _demografr_ scaffold model is to write a standard _slendr_ model the usual way, manually:

```{r, slendr_model2, fig.width=5, fig.height=3, fig.align="center"}
popA <- population("popA", time = 1, N = 1)
popB <- population("popB", time = 2500, N = 1, parent = popA)
popC <- population("popC", time = 5000, N = 1, parent = popB)
popD <- population("popD", time = 7500, N = 1, parent = popC)

model2 <- compile_model(
  populations = list(popA, popB, popC, popD),
  simulation_length = 10000, generation_time = 1, serialize = FALSE
)

plot_model(model2)
```

Note that the visualization shows the same model as the one we defined with an input tree above. This is not a surprise, because what the automated functions `tree_model()` does internally is run exactly the same sequence of commands we specified manually above.

Why would you use this option instead of inputting a phylogenetic tree? For one thing, this method gives you a finer control over which population should exactly split from which, which gets a little tricky with the Newick tree input method via `tree_model()` or `tree_populations()`. Additionally, it makes it possible to fix some model parameters (such as $N_e$ or divergence times) to different values, skipping the need to specify prior distributions for them simply by leaving them out of the prior definition list.


```{r}
validate_abc(model2, priors, functions, observed)
```

Again, we will skip the simulations and inference, which we would normally perform with something like this (skipping this step here to save computational time):

```{r, eval=FALSE}
# first simulate data
data <- simulate_abc(model2, priors, functions, observed, iterations = 10000,
                    sequence_length = 1e6, recombination_rate = 1e-8)

# then perform ABC inference
abc <- perform_abc(data, tolerance = 0.05, method = "neuralnet")
```







### Function-based model definition

A yet another way to define a _demografr_ scaffold model which allows even more flexibility in terms of supporting complex parametrizations is to provide an entire self-contained function:

```{r}
custom_model <- function(size_A, size_B, size_C, size_D, split_AB, split_BC, split_CD) {
  popA <- population("popA", time = 1,        N = size_A)
  popB <- population("popB", time = split_AB, N = size_B, parent = popA)
  popC <- population("popC", time = split_BC, N = size_C, parent = popB)
  popD <- population("popD", time = split_CD, N = size_D, parent = popC)

  compile_model(
    populations = list(popA, popB, popC, popD),
    simulation_length = 10000, generation_time = 1, serialize = FALSE
  )
}
```

Hopefully, it should be clear how much more flexibility this kind of set up gives us in defining scaffold models for an ABC analysis. Of course, running this function with a given set of parameters compiles a standard _slendr_ model, no surprises there.  This makes it easy to verify that the generating function is set up correctly, like we do in the following code chunk by providing arbitrary values of our parameters:

```{r, slendr_model3, fig.width=5, fig.height=3, fig.align="center"}
test_model <- custom_model(
  size_A = 1123, size_B = 4321, size_C = 42, size_D = 2000,
  split_AB = 500, split_BC = 5000, split_CD = 7000
)

plot_model(test_model)
```

How do we use this kind of flexible scaffold model for ABC inference? First, we need to specify the priors. This is very easy: the only thing we need to do is **specify prior parameter sampling statements with variable names matching the function arguments!**

```{r}
custom_priors <- list(
  size_A ~ runif(1, 10000),
  size_B ~ runif(1, 10000),
  size_C ~ runif(1, 10000),
  size_D ~ runif(1, 10000),

  split_AB ~ runif(1, 3000),
  split_BC ~ runif(3000, 6000),
  split_CD ~ runif(6000, 9000)
)
```

Note that the visualization shows the same model. This is not a surprise, because what the automated functions `tree_model()` does internally is run exactly the same sequence of commands we specified manually above.

For such generating function-based models, it is even more worth to validate the ABC setup to catch issues as early as possible:

```{r}
validate_abc(custom_model, custom_priors, functions, observed)
```

Having convinced ourselves that the ABC model components are correctly configured, we would proceed with inference (again, we're not going to run the full ABC pipeline here because it would take a lot of time to get useful posteriors):


```{r, eval=FALSE}
# first simulate data
data <- simulate_abc(custom_model, custom_priors, functions, observed, iterations = 10000,
                     sequence_length = 1e6, recombination_rate = 1e-8)

# then perform ABC inference
abc <- perform_abc(data, tolerance = 0.05, method = "neuralnet")
```

