---
title: "Expected computation times"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vignette-01-overview}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

```{r setup}
library(microbenchmark)
library(ggplot2)
library(dplyr)
library(slendr)
```

## Introduction

An important thing to consider in simulation-heavy procedures such as ABC is the trade-off between computing time and accuracy. Intuitively, the larger the amount of sequence data simulated, the lower the statistical noise in the estimated summary statistics; the larger the number of simulation replicates, the more accurate posterior distributions of parameters of interest. However, it is difficult to predict _a priori_ how much computing time would a tree-sequence simulation from particular combination of sequence length&ndash;size of the population(s)&ndash;number of replicates take to run. This vignette aims to provide a rough set of guidelines to establish reasonable runtimes of tree-sequence simulations and analyses. We will focus on simulations of _slendr_ models but because we will use its _msprime_ simulation back end to generate and analyse tree sequence data, the results should be applicable more generally.

## Single-population simulation setup

First, let's create a trivially simple _slendr_ model involving a single population:

```{r}
model <- compile_model(
    populations = population("pop", N = 10000, time = 1),
    generation_time = 1, simulation_length = 1000
)
```

We want to benchmark tree-sequence simulation across a wide range of sequence lengths (from 10kb to 1Gb). First, let's collect the simulation calls in an R list. Note that we're using a `bquote` metaprogramming "trick" because we don't want to run any simulations just yet&mdash;the code chunk bellow only prepares a list of function calls to benchmark:

```{r}
# generate a numeric vector of sequence lengths from 10kb to 1Gb
sequence_lengths <-
  sapply(c(1, 10, 100, 1000, 10000), function(scale)
    sapply(seq(10e3, 90e3, by = 10e3), function(base) scale * base)
  ) %>%
  as.vector %>% c(., 1e9)

# generate a list of calls to the msprime function (just the call expressions, not evaluations!) 
runs <- lapply(sequence_lengths, function(x)
               bquote(msprime(model, sequence_length = .(x), recombination_rate = 1e-8)))

# assign readable labels to each simulation run for easier plotting later
name_unit <- function(x) {
  if      (x < 1e3)     x
  else if (x / 1e6 < 1) paste(x / 1e3, "kb")
  else if (x / 1e9 < 1) paste(x / 1e6, "Mb")
  else                  paste(x / 1e9, "Gb")
}

names(runs) <- sapply(sequence_lengths, name_unit)
```

Now we have the list of simulation calls ready for benchmarking. Let's take a look at the structure of the data, just to get an idea what we've done above:

```{r}
runs[1:5]
```

## benchmarking

We will run five replicates of each simulation and measure the time it takes to run for different sequence lengths:

```{r}
Sys.time()
times <- microbenchmark(list = runs, times = 1, unit = "seconds")
Sys.time()

saveRDS(times, "inst/extdata/simulation_benchmark.rds")
```

Let's define a simple function which converts the benchmarking results into a normal data frame in a long format:

```{r}
df <- as_tibble(times) %>%
  rename(run = expr) %>%
  mutate(
    time = time / 1000000000,
    seq = factor(gsub("seq", "", run), levels = gsub("seq", "", levels(run)))
  )
```

```{r}
ggplot(df, aes(seq, time)) +
  geom_violin() +
  labs(y = "time [seconds]") +
  coord_flip() +
  theme_minimal()
```