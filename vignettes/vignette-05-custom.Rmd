---
title: Custom inference using SLiM or Python
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Custom inference using SLiM or Python}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
slendr_present <- slendr::check_dependencies(python = TRUE)

knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#>",
  fig.align = "center",
  fig.width = 8,
  fig.height = 5,
  dpi = 80,
  eval = slendr_present
)

abc_path1 <- here::here("inst/examples/custom_abc1.rds")
abc_path2 <- here::here("inst/examples/custom_abc2.rds")

devtools::install_github("bodkan/demografr", upgrade = FALSE)
devtools::install_github("bodkan/slendr", upgrade = FALSE)
```

⚠️⚠️⚠️

**Note:** The _demografr_ R package is still under active development. As a
result, its documentation is in a draft stage at best. Typos, inconsistencies,
and other issues are unfortunately expected.

⚠️⚠️⚠️

By default, _demografr_ uses the [_slendr_](https://github.com/bodkan/slendr)
package for defining models and simulating data from them. But what if you need
to do inference using your own scripts? Perhaps _slendr_'s opinionated
interface doesn't allow you to do all that you need to do, such as using some
of the powerful simulation features and options of "raw"
[SLiM](https://messerlab.org/slim/) or
[_msprime_](https://tskit.dev/msprime/docs/stable/intro.html)? Alternatively,
perhaps you don't want to compute summary statistics (just) with _slendr_'s
tree-sequence interface to _tskit_ (you might want to compute statistics using
other software), or you want to compute statistics with fully customized
Python functions. This vignette
explains how you can use any standard SLiM or _msprime_ script as a simulation
engine in a standard _demografr_ pipeline without _slendr_, as well as how to
utilize customized, non-_slendr_ summary statistic functions.

First, let's load _demografr_ itself and also _slendr_ (which, in this
vignette, will serve only for working with simulated tree sequences, but not
simulations themselves).

```{r}
library(demografr)
library(slendr)
init_env(quiet = TRUE)
```

## Toy inference problem

Suppose that we intent to use ABC to infer the $N_e$ of a constant-sized
population given that we observed the following value of nucleotide diversity:

```{r, echo=FALSE}
ts <- population("pop", N = 1000, time = 10000) %>%
  compile_model(generation_time = 1, direction = "backward") %>%
  msprime(sequence_length = 200e6, recombination_rate = 1e-8, random_seed = 42) %>%
  ts_mutate(mutation_rate = 1e-8, random_seed = 42)
observed_diversity <- ts_diversity(ts, list(pop = ts_samples(ts)$name))
```

```{r}
observed_diversity
```

We want to infer the posterior distribution of $N_e$. In this vignette, we're
going to show how to accomplish first not via the normal
_slendr_ interface (as shown [here](vignette-01-basics.html)) but using a
simple SLiM or Python script. In a later section, we're also going to
use this example to demonstrate how to compute population genetic summary
statistic (like nucleotide diversity) using external software.

**We acknowledge that this is a completely trivial example, not really worth
spending so much effort on doing an ABC for.** The example was chosen because
it runs fast and demonstrates all the features of _demografr_ that you would
use regardless of the complexity of your model.

## Using custom scripts as simulation engines

In order to be able to use a custom SLiM or msprime script with _demografr_,
the script must conform to a couple of rules:

### 1. It must be runnable on the command-line as any other command-line script

- **For an _msprime_ script**, this means something like `python <your script>.py <command-line arguments>`.

- **For a SLiM script**, this means something like `slim <command-line arguments> <your script>.slim`.

### 2. It must accept `--path` argument pointing to a directory where it will save output files

This is analogous to the `path =` argument of _slendr_ functions `slim()` and `msprime()`.

**For an _msprime_ script**, this parameter can be specified via the Python
built-in module `argparse`, and provided on the command-line as
`--path <path to directory>`. In the script itself, if you use the `argparse`
module and have thus the values of provided arguments (for instance) in an
`args` object, you can then refer to these arguments as `args.path`, etc.
Given that you're reading this, I assume you know what the above information
means, but here's a [link to the relevant section](https://docs.python.org/3/library/argparse.html)
of the Python documentation for completeness.

**For a SLiM script**, this parameter can be specified via SLiM's standard way
of supplying command-line arguments as `-d "path='<path to a directory>'"`.
Importantly, note SLiM's format of specifying string arguments for the `path`
argument. If you need further detail, see Section 20.2 of the SLiM manual.
In the script itself, you can then refer to this argument via the constant
`path`.

### 3. All model parameters must be provided as additional command-line arguments

You can refer to them in your script as you would to the mandatory arguments
as described in the previous section.

**A useful check to see whether you script would make a valid engine for a
_demografr_ inference pipeline is to run it manually on the command line like
this:**

```
python \
  <path to your Python script>                   \
  --path <path to a directory> \
  <... your model parameters ...>
```

Or, if you want to run a SLiM-powered ABC, like this:

```
slim \
  -d "path='<path to a directory>'" \
  <... your model parameters ...>
```

Then, if you look into the directory and see the desired files produced by your
simulation script, you're good to go!

**Below you are going to see how to actually read back simulation results from
disk and use them for computing summary statistics. For now, just note that you
can name the files produced by your simulation script however you'd like and
can have as many as you'd like, as long as they are all in that one directory
given by `path`.**

### Example pure SLiM and msprime script

To demonstrate the requirements 1-3 above in practice, let's run an ABC
inference using an example _msprime_ script and a SLiM script as
simulation engines.

Imagine we have the following two scripts which we want to use as simulation
engines instead of _slendr_'s own functions `slim()` and `msprime()`.
The scripts have only **one _model parameter_, $N_e$ of the single modeled
population**. The **other parameters are mandatory**, just discussed in the
previous section.

#### SLiM script

```{r}
slim_script <- system.file("examples/custom.slim", package = "demografr")
```

```{r, comment="", echo=FALSE}
cat(readLines(slim_script), sep = "\n")
```

#### Python script

```{r}
python_script <- system.file("examples/custom.py", package = "demografr")
```

```{R, comment="", echo=FALSE}
cat(readLines(python_script), sep = "\n")
```

**Note that the Python script specifies all model parameters (here just $N_e$)
and mandatory arguments via a command-line interface provided by the Python
module `argparse`**.

**The SLiM script, on the other hand, uses SLiM's features for command-line
specification of parameters and simply refers to each parameter by its symbolic
name.** No other setup is necessary.

Please also note that given that there are can be discrepancies between values
of arguments of some SLiM or Python methods (such as `addSubPop` of SLiM which
expects an integer value for a population size, or the `samples` argument of
`msprime.sim_ancestry`), and values of parameters sampled from priors by
_demografr_ (i.e., $N_e$ often being a floating-point value after sampling from
a continuous prior), you might have to perform explicit type conversion in your
custom scripts (such as `sim.addSubPop("p0", asInteger(Ne)` as above).

### ABC inference using custom _msprime_ script

Apart from the user-defined simulation SLiM and _msprime_ scripts, the
components of our toy inference remains the same&mdash;we need to define the
observed statistics, tree-sequence summary functions, and priors. We don't need
a model function&mdash;that will be served by our custom script.

Here are the _demografr_ pipeline components (we won't be discussing them here
because that's extensively taken care of [elsewhere](vignette-01-basics.html)
in _demografr_'s documentation vignettes):

```{r}
# a single prior parameter
priors <- list(Ne ~ runif(100, 5000))

# a single observed statistic
observed <- list(diversity = observed_diversity)

# compute diversity using 100 chromosomes
compute_diversity <- function(ts) {
  ts_diversity(ts, list(pop = seq(0, 99)))
}
functions <- list(diversity = compute_diversity)

# data-generating functions for computing summary statistics
gens <- list(
  ts = function(path) ts_read(file.path(path, "result.trees"))
)
```

(Note that because our simulated tree sequences won't be coming with _slendr_
metadata, we have to refer to individuals' chromosomes using numerical indices
rather than _slendr_ symbolic names like we do in all of our other vignettes.)

#### Simulating a testing tree sequence

As we explained elsewhere, a useful function for developing inference pipelines
using _demografr_ is a function `simulate_model()`, which normally accepts a
_slendr_ model generating function and the model parameters (either given as
priors or as a list of named values), and simulates a tree-sequence object.
This function (as any other _demografr_ function operating with models) accepts
our custom-defined simulation scripts in place of standard _slendr_ models.

For instance, we can simulate a couple of Mb of testing sequence from our
_msprime_ script like this:

```{r}
result_msprime <- simulate_model(python_script, parameters = list(Ne = 123), format = "files", data = gens)
```

Now we can finally test our toy tree-sequence summary function, verifying that
we can indeed compute the summary statistic we want to:

```{r}
functions$diversity(result_msprime$ts)
```

The function works as expected&mdash;we have a single population and want to
compute nucleotide diversity in the whole population, and this is exactly what
we get.

```{r, echo=FALSE}
result_slim <- simulate_model(slim_script, parameters = list(Ne = 123), format = "files", data = gens)

functions$diversity(result_slim$ts)
```

Note that we are also able to use a dedicated function for this, called
`summarise_data()`, which takes in a product of a simulation (tree sequence
and/or a path to a directory with simulation results), and apply all summary
functions to this data:

```{r}
summarise_data(result_msprime, functions)
summarise_data(result_slim, functions)
```


#### ABC inference

Having all components of our pipeline set up we should, again, validate
everything before we proceed to (potentially very costly) simulations. In the
remainder of this vignette we'll only continue with the Python msprime custom
script in order to save some computational time. That said, it's important to
realize that you could use this sort of workflow for any kind of SLiM script,
including very elaborate spatial simulations, non-WF models, and all kinds of
phenotypic simulations too! **Although primarily designed to work with
_slendr_, the _demografr_ package intends to fully support any kind of SLiM or
_msprime_ simulation. If something doesn't work, consider it a _demografr_
bug!**

Let's first validate all components of our pipeline:

```{r}
validate_abc(python_script, priors, functions, observed, data = gens, format = "files")
```

```{r, echo=FALSE, results='hide'}
validate_abc(slim_script, priors, functions, observed, data = gens, format = "files")
```

Looking good! Now let's first run ABC simulations. Again, note the use of the
Python script where we would normally provide a _slendr_ model function in
place of the `model` argument:

```{r, echo=FALSE, eval=TRUE}
tstart <- Sys.time()
```

```{r, eval=!file.exists(abc_path1)}
library(future)
plan(multisession, workers = availableCores()) # parallelize across all CPUs

data <- simulate_abc(
  model = python_script, priors, functions, observed, iterations = 10000,
  data = gens, format = "files"
)
```

```{r, echo=FALSE, eval=TRUE}
tend <- Sys.time()
tdelta <- as.numeric(difftime(tend, tstart, units = "secs"))
ncores <- future::availableCores()
```

```{r, echo=FALSE, eval=!file.exists(abc_path1)}
saveRDS(tdelta, here::here("inst/examples/custom_tdelta1.rds"))
saveRDS(ncores, here::here("inst/examples/custom_ncores1.rds"))
```

```{r, echo=FALSE, eval=file.exists(abc_path1)}
tdelta <- readRDS(here::here("inst/examples/custom_tdelta1.rds"))
ncores <- readRDS(here::here("inst/examples/custom_ncores1.rds"))
```

```{r, echo=FALSE, eval=TRUE}
hours <- floor(tdelta / 3600)
minutes <- floor((tdelta - hours * 3600) / 60)
seconds <- round(tdelta - hours * 3600 - minutes * 60)
```

**The total runtime for the ABC simulations was
`r paste(hours, "hours", minutes, "minutes", seconds, "seconds")`
parallelized across `r ncores` CPUs.**

Once the simulations are finished, we can perform inference of the posterior
distribution of the single parameter of our model, $N_e$:

```{r, eval=!file.exists(abc_path1)}
abc <- run_abc(data, engine = "abc", tol = 0.01, method = "neuralnet")
```

```{r, echo=FALSE, eval=!file.exists(abc_path1)}
saveRDS(abc, abc_path1)
```

```{r, echo=FALSE, eval=file.exists(abc_path1)}
abc <- readRDS(abc_path1)
```

Having done so, we can again look at the summary statistics of the posterior
distribution and also plot the results (we're skipping diagnostics such as
posterior predictive checks but you can read more about those
[here](vignette-01-basics.html) and [here](vignette-06-diagnostics.html)).
Because this `observed` diversity is based on simulated data from a known
model, we'll indicate the true "hidden" $N_e$ value with a vertical line.

```{r}
extract_summary(abc)
```

```{r, custom_Ne_posterior1}
library(ggplot2)

plot_posterior(abc) +
  geom_vline(xintercept = 1000, linetype = "dashed") +
  coord_cartesian(xlim = c(100, 5000))
```

## Using external programs to compute summary statistics

Now let's move things one step forward. What if we either don't find _slendr_'s
tree-sequence interface to _tskit_ sufficient and want to compute summary
statistics from the simulated data using some external software, like PLINK or
EIGENSTRAT?

To keep things as easy as possible (and avoid dragging in multiple software
dependencies), let's say we have a program `vcf_diversity`, which is run in
the following way:

```
./vcf_diversity --vcf <path to a VCF> --tsv <path to a TSV table>
```

This program takes in a VCF file and computes the nucleotide diversity across
all individuals in that VCF file. For simplicity, it produces a table of
results in exactly the same format as the `observed_diversity` data frame
above.

In this particular example, we want to replace the computation of statistics
performed by the R function `compute_diversity` above (which also produces
a data frame) with the product of the external software `vcf_diversity`.

(We provide this `vcf_diversity` program as an example script, but it could
of course be any other program you could imagine.)

```{r}
reticulate::py_install("scikit-allel", pip = TRUE)
```

```{r}
# a single prior parameter
priors <- list(Ne ~ runif(100, 5000))

# a single observed statistic
observed <- list(diversity = observed_diversity)

# a single tree-sequence summary function
compute_diversity1 <- function(vcf) {
  python <- reticulate::py_config()$python
  program <- system.file("examples/custom_diversity", package = "demografr")
  program <- "/project/inst/examples/custom_diversity"
  tsv <- tempfile()

  cmd <- paste(python, program, "--vcf", vcf, "--tsv", tsv)
  system(cmd)

  # read the computed table with nucleotide diversity in the simulated# VCF
  read.table(tsv, header = TRUE)
}

# a single tree-sequence summary function
compute_diversity2 <- function(gt) {
  gt <- gt[-1]
  pi <- c()
  for (i in 1:(ncol(gt) - 1)) {
    for (j in (i + 1):ncol(gt)) {
       pi <- c(pi, mean(gt[[i]] != gt[[j]]))
    }
  }
  mean(pi) / 1e6
}

# a single tree-sequence summary function
compute_diversity2_ <- function(gt) {
  start <- gt$pos[1]
  end <- gt$pos[nrow(gt)]
  gt <- gt[, -1]
  diffs <- c()
  for (i in 1:(ncol(gt) - 1)) {
    for (j in (i + 1):ncol(gt)) {
       diffs <- c(diffs, sum(gt[[i]] != gt[[j]]))
    }
  }
  mean(diffs) / (end - start + 1)
  mean(diffs) / 1e6
}

compute_diversity3 <- function(ts) {
  ts_diversity(ts, list(pop = seq(0, 99)))
}

compute_pairwise_pi <- function(gt) {
  chromosome_pairs <- combn(names(gt), 2, simplify = FALSE)  # Generate all chromosome pairs
  num_sites <- nrow(gt)  # Number of SNP sites
  results <- data.frame(Chromosome1 = character(), Chromosome2 = character(), NucleotideDiversity = numeric())

  for (pair in chromosome_pairs) {
    chr1 <- pair[1]
    chr2 <- pair[2]
    
    # Compute the number of differing sites
    differences <- sum(gt[[chr1]] != gt[[chr2]])
    
    # Compute nucleotide diversity (pi)
    pi_value <- differences / num_sites
    
    # Store results
    results <- results %>% tibble::add_row(Chromosome1 = chr1, Chromosome2 = chr2, NucleotideDiversity = pi_value)
  }
  
  return(mean(results$NucleotideDiversity))
}

functions <- list(
  compute_diversity1 = compute_diversity1,
  compute_diversity2 = compute_diversity2,
  compute_diversity2_ = compute_diversity2_,
  compute_diversity3 = compute_diversity3,
  compute_pairwise_pi = compute_pairwise_pi
)

# data-generating functions for computing summary statistics
gens <- list(
  vcf = function(path) {
    # read the simulated tree sequence from a given path and subset it
    ts_path <- file.path(path, "result.trees")
    ts <- ts_read(ts_path) %>% ts_simplify(simplify_to = seq(0, 99))

    # write a VCF to the same directory
    vcf_path <- file.path(path, "genotypes.vcf.gz")
    ts_vcf(ts, vcf_path)

    # return the path to the VCF for use in computing summary statistics
    return(vcf_path)
  },
  gt = function(path) {
    # read the simulated tree sequence from a given path and subset it
    ts_path <- file.path(path, "result.trees")
    ts <- ts_read(ts_path) %>% ts_simplify(simplify_to = seq(0, 99))
    ts_genotypes(ts)
  },
  ts = function(path) {
    ts_path <- file.path(path, "result.trees")
    ts <- ts_read(ts_path) %>% ts_simplify(simplify_to = seq(0, 99))
    ts
  }
)
```

```{r}
result_msprime <- simulate_model(python_script, parameters = list(Ne = 123), format = "files", data = gens)
```

```{r}
summarise_data(result_msprime, functions)
```

```{r, echo=FALSE, eval=TRUE}
tstart <- Sys.time()
```

```{r, eval=!file.exists(abc_path2)}
library(future)
plan(multisession, workers = availableCores()) # parallelize across all CPUs

data <- simulate_abc(
  model = python_script, priors, functions, observed, iterations = 10000,
  data = gens, format = "files"
)
```

```{r, echo=FALSE, eval=TRUE}
tend <- Sys.time()
tdelta <- as.numeric(difftime(tend, tstart, units = "secs"))
ncores <- future::availableCores()
```

```{r, echo=FALSE, eval=!file.exists(abc_path2)}
saveRDS(tdelta, here::here("inst/examples/custom_tdelta2.rds"))
saveRDS(ncores, here::here("inst/examples/custom_ncores2.rds"))
```

```{r, echo=FALSE, eval=file.exists(abc_path2)}
tdelta <- readRDS(here::here("inst/examples/custom_tdelta2.rds"))
ncores <- readRDS(here::here("inst/examples/custom_ncores2.rds"))
```

```{r, echo=FALSE, eval=TRUE}
hours <- floor(tdelta / 3600)
minutes <- floor((tdelta - hours * 3600) / 60)
seconds <- round(tdelta - hours * 3600 - minutes * 60)
```

**The total runtime for the ABC simulations was
`r paste(hours, "hours", minutes, "minutes", seconds, "seconds")`
parallelized across `r ncores` CPUs.**

Once the simulations are finished, we can perform inference of the posterior
distribution of the single parameter of our model, $N_e$:

```{r, eval=!file.exists(abc_path2)}
abc <- run_abc(data, engine = "abc", tol = 0.01, method = "neuralnet")
```

```{r, echo=FALSE, eval=!file.exists(abc_path2)}
saveRDS(abc, abc_path2)
```

```{r, echo=FALSE, eval=file.exists(abc_path2)}
abc <- readRDS(abc_path2)
```
```{r}
extract_summary(abc)
```

```{r, custom_Ne_posterior2}
library(ggplot2)

plot_posterior(abc) +
  geom_vline(xintercept = 1000, linetype = "dashed") +
  coord_cartesian(xlim = c(100, 5000))
```
