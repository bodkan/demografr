---
title: Custom inference using SLiM or Python
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Custom inference using SLiM or Python}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
slendr_present <- slendr::check_dependencies(python = TRUE, slim = TRUE)
on_windows <- Sys.info()[["sysname"]] == "Windows"
rerun <- Sys.getenv("RERUN") == "TRUE"

get_cache <- function(path) {
  if (Sys.getenv("RERUN") == "TRUE")
    return(here::here(file.path("inst/", path)))
  else
    return(file.path(system.file(path, package = "demografr")))
}

path_observed <- get_cache("examples/custom_diversity.rds")
abc_path1 <- get_cache("examples/custom_abc1.rds")
abc_path2 <- get_cache("examples/custom_abc2.rds")
abc_path3 <- get_cache("examples/custom_abc3.rds")
abc_path4 <- get_cache("examples/custom_abc4.rds")

knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#>",
  fig.align = "center",
  fig.width = 8,
  fig.height = 5,
  dpi = 60,
  eval = slendr_present && !on_windows
)
```

⚠️⚠️⚠️

**Please note that until a peer reviewed publication describing the
_demografr_ package is published, the software should be regarded as 
experimental and potentially unstable.**

⚠️️⚠️⚠️

By default, _demografr_ uses the [_slendr_](https://github.com/bodkan/slendr)
package for defining models and simulating data from them. But what if you need
to do inference using your own scripts? Perhaps _slendr_'s opinionated
interface doesn't allow you to do all that you need to do, such as using some
of the powerful simulation features and options of "raw"
[SLiM](https://messerlab.org/slim/) or
[_msprime_](https://tskit.dev/msprime/docs/stable/intro.html)? Alternatively,
perhaps you don't want to compute summary statistics (just) with _slendr_'s
tree-sequence interface to _tskit_ (you might want to compute statistics using
other software), or you want to compute statistics with fully customized
Python functions. This vignette
explains how you can use any standard SLiM or _msprime_ script as a simulation
engine in a standard _demografr_ pipeline without _slendr_, as well as how to
utilize customized, non-_slendr_ summary statistic functions.

First, let's load _demografr_ itself and also _slendr_ (which, in this
vignette, will serve only for working with simulated tree sequences, but not
simulations themselves).

```{r}
library(demografr)
library(slendr)
init_env(quiet = TRUE)
```

## Toy inference problem

Suppose that we intent to use ABC to infer the $N_e$ of a constant-sized
population given that we observed the following value of nucleotide diversity:

```{r, echo=FALSE, eval=!on_windows && !file.exists(path_observed) && rerun}
ts <- population("pop", N = 1000, time = 10000) %>%
  compile_model(generation_time = 1, direction = "backward") %>%
  msprime(sequence_length = 200e6, recombination_rate = 1e-8, random_seed = 42) %>%
  ts_mutate(mutation_rate = 1e-8, random_seed = 42)
observed_diversity <- ts_diversity(ts, list(pop = ts_samples(ts)$name))
saveRDS(observed_diversity, path_observed)
```

```{r, echo=FALSE, eval=file.exists(path_observed)}
observed_diversity <- readRDS(path_observed)
```

```{r}
observed_diversity
```

We want to infer the posterior distribution of $N_e$. In this vignette, we're
going to show how to accomplish first not via the normal
_slendr_ interface (as shown [here](vignette-01-basics.html)) but using a
simple SLiM or Python script. In a later section, we're also going to
use this example to demonstrate how to compute population genetic summary
statistic (like nucleotide diversity) using external software.

**We acknowledge that this is a completely trivial example, not really worth
spending so much effort on doing an ABC for.** The example was chosen because
it runs fast and demonstrates all the features of _demografr_ that you would
use regardless of the complexity of your model.

## Using custom scripts as simulation engines

In order to be able to use a custom SLiM or msprime script with _demografr_,
the script must conform to a couple of rules:

### 1. It must be runnable on the command-line as any other command-line script

- **For an _msprime_ script**, this means something like `python <your script>.py <command-line arguments>`.

- **For a SLiM script**, this means something like `slim <command-line arguments> <your script>.slim`.

### 2. It must accept `--path` argument pointing to a directory where it will save output files

This is analogous to the `path =` argument of _slendr_ functions `slim()` and `msprime()`.

**For an _msprime_ script**, this parameter can be specified via the Python
built-in module `argparse`, and provided on the command-line as
`--path <path to directory>`. In the script itself, if you use the `argparse`
module and have thus the values of provided arguments (for instance) in an
`args` object, you can then refer to these arguments as `args.path`, etc.
Given that you're reading this, I assume you know what the above information
means, but here's a [link to the relevant section](https://docs.python.org/3/library/argparse.html)
of the Python documentation for completeness.

**For a SLiM script**, this parameter can be specified via SLiM's standard way
of supplying command-line arguments as `-d "path='<path to a directory>'"`.
Importantly, note SLiM's format of specifying string arguments for the `path`
argument. If you need further detail, see Section 20.2 of the SLiM manual.
In the script itself, you can then refer to this argument via the constant
`path`.

### 3. All model parameters must be provided as additional command-line arguments

You can refer to them in your script as you would to the mandatory arguments
as described in the previous section.

**A useful check to see whether you script would make a valid engine for a
_demografr_ inference pipeline is to run it manually on the command line like
this:**

```
python \
  <path to your Python script>                   \
  --path <path to a directory> \
  <... your model parameters ...>
```

Or, if you want to run a SLiM-powered ABC, like this:

```
slim \
  -d "path='<path to a directory>'" \
  <... your model parameters ...>
```

Then, if you look into the directory and see the desired files produced by your
simulation script, you're good to go!

**Below you are going to see how to actually read back simulation results from
disk and use them for computing summary statistics. For now, just note that you
can name the files produced by your simulation script however you'd like and
can have as many as you'd like, as long as they are all in that one directory
given by `path`.**

### Example pure SLiM and msprime script

To demonstrate the requirements 1-3 above in practice, let's run an ABC
inference using an example _msprime_ script and a SLiM script as
simulation engines.

Imagine we have the following two scripts which we want to use as simulation
engines instead of _slendr_'s own functions `slim()` and `msprime()`.
The scripts have only **one _model parameter_, $N_e$ of the single modeled
population**. The **other parameters are mandatory**, just discussed in the
previous section.

#### SLiM script

```{r}
slim_script <- system.file("examples/custom.slim", package = "demografr")
```

```{r, comment="", echo=FALSE}
cat(readLines(slim_script), sep = "\n")
```

#### Python script

```{r}
python_script <- system.file("examples/custom.py", package = "demografr")
```

```{R, comment="", echo=FALSE}
cat(readLines(python_script), sep = "\n")
```

**Note that the Python script specifies all model parameters (here just $N_e$)
and mandatory arguments via a command-line interface provided by the Python
module `argparse`**.

**The SLiM script, on the other hand, uses SLiM's features for command-line
specification of parameters and simply refers to each parameter by its symbolic
name.** No other setup is necessary.

Please also note that given that there are can be discrepancies between values
of arguments of some SLiM or Python methods (such as `addSubPop` of SLiM which
expects an integer value for a population size, or the `samples` argument of
`msprime.sim_ancestry`), and values of parameters sampled from priors by
_demografr_ (i.e., $N_e$ often being a floating-point value after sampling from
a continuous prior), you might have to perform explicit type conversion in your
custom scripts (such as `sim.addSubPop("p0", asInteger(Ne)` as above).

### ABC inference using custom _msprime_ script

Apart from the user-defined simulation SLiM and _msprime_ scripts, the
components of our toy inference remains the same&mdash;we need to define the
observed statistics, tree-sequence summary functions, and priors. We don't need
a model function&mdash;that will be served by our custom script.

Here are the _demografr_ pipeline components (we won't be discussing them here
because that's extensively taken care of [elsewhere](vignette-01-basics.html)
in _demografr_'s documentation vignettes):

```{r}
# a single prior parameter
priors <- list(Ne ~ runif(100, 5000))

# a single observed statistic
observed <- list(diversity = observed_diversity)

# compute diversity using 100 chromosomes
compute_diversity_ts <- function(ts) {
  samples <- ts$samples()
  ts_diversity(ts, list(pop = sample(samples, 100)))
}
functions <- list(diversity = compute_diversity_ts)

# data-generating functions for computing summary statistics
gens <- list(
  ts = function(path) ts_read(file.path(path, "result.trees"))
)
```

(Note that because our simulated tree sequences won't be coming with _slendr_
metadata, we have to refer to individuals' chromosomes using numerical indices
rather than _slendr_ symbolic names like we do in all of our other vignettes.)

#### Simulating a testing tree sequence

As we explained elsewhere, a useful function for developing inference pipelines
using _demografr_ is a function `simulate_model()`, which normally accepts a
_slendr_ model generating function and the model parameters (either given as
priors or as a list of named values), and simulates a tree-sequence object.
This function (as any other _demografr_ function operating with models) accepts
our custom-defined simulation scripts in place of standard _slendr_ models.

For instance, we can simulate a couple of Mb of testing sequence from our
_msprime_ script like this:

```{r}
model_run <- simulate_model(python_script, parameters = list(Ne = 123), format = "files", data = gens)
```

Now we can finally test our toy tree-sequence summary function, verifying that
we can indeed compute the summary statistic we want to:

```{r}
functions$diversity(model_run$ts)
```

The function works as expected&mdash;we have a single population and want to
compute nucleotide diversity in the whole population, and this is exactly what
we get.

```{r, echo=FALSE}
result_slim <- simulate_model(slim_script, parameters = list(Ne = 123), format = "files", data = gens)

functions$diversity(result_slim$ts)

summarise_data(result_slim, functions)
```

Note that we are also able to use a dedicated function for this, called
`summarise_data()`, which takes in a product of a simulation (tree sequence
and/or a path to a directory with simulation results), and apply all summary
functions to this data:

```{r}
summarise_data(model_run, functions)
```


#### ABC inference

Having all components of our pipeline set up we should, again, validate
everything before we proceed to (potentially very costly) simulations. In the
remainder of this vignette we'll only continue with the Python msprime custom
script in order to save some computational time. That said, it's important to
realize that you could use this sort of workflow for any kind of SLiM script,
including very elaborate spatial simulations, non-WF models, and all kinds of
phenotypic simulations too! **Although primarily designed to work with
_slendr_, the _demografr_ package intends to fully support any kind of SLiM or
_msprime_ simulation. If something doesn't work, consider it a _demografr_
bug!**

Let's first validate all components of our pipeline:

```{r}
validate_abc(python_script, priors, functions, observed, data = gens, format = "files")
```

```{r, echo=FALSE, results='hide'}
validate_abc(slim_script, priors, functions, observed, data = gens, format = "files")
```

Looking good! Now let's first run ABC simulations. Again, note the use of the
Python script where we would normally provide a _slendr_ model function in
place of the `model` argument:

```{r, echo=FALSE, eval=TRUE}
tstart <- Sys.time()
```

```{r, eval=!on_windows && !file.exists(abc_path1) && rerun}
library(future)
plan(multisession, workers = availableCores()) # parallelize across all CPUs

data <- simulate_abc(
  model = python_script, priors, functions, observed, iterations = 10000,
  data = gens, format = "files"
)
```

```{r, echo=FALSE}
tend <- Sys.time()
tdelta <- as.numeric(difftime(tend, tstart, units = "secs"))
ncores <- future::availableCores()
```

```{r, echo=FALSE, eval=!on_windows && !file.exists(abc_path1) && rerun}
saveRDS(tdelta, get_cache("examples/custom_tdelta1.rds"))
saveRDS(ncores, get_cache("examples/custom_ncores1.rds"))
```

```{r, echo=FALSE, eval=TRUE}
tdelta <- readRDS(get_cache("examples/custom_tdelta1.rds"))
ncores <- readRDS(get_cache("examples/custom_ncores1.rds"))
```

```{r, echo=FALSE, eval=TRUE}
hours <- floor(tdelta / 3600)
minutes <- floor((tdelta - hours * 3600) / 60)
seconds <- round(tdelta - hours * 3600 - minutes * 60)
```

**The total runtime for the ABC simulations was
`r paste(hours, "hours", minutes, "minutes", seconds, "seconds")`
parallelized across `r ncores` CPUs.**

Once the simulations are finished, we can perform inference of the posterior
distribution of the single parameter of our model, $N_e$:

```{r, eval=!on_windows && !file.exists(abc_path1)}
abc <- run_abc(data, tol = 0.01, method = "neuralnet")
```

```{r, echo=FALSE, eval=!on_windows && !file.exists(abc_path1)}
saveRDS(abc, abc_path1)
```

```{r, echo=FALSE, eval=file.exists(abc_path1)}
abc <- readRDS(abc_path1)
```

Having done so, we can again look at the summary statistics of the posterior
distribution and also plot the results (we're skipping diagnostics such as
posterior predictive checks but you can read more about those
[here](vignette-01-basics.html) and [here](vignette-06-diagnostics.html)).
Because this `observed` diversity is based on simulated data from a known
model, we'll indicate the true "hidden" $N_e$ value with a vertical line.

```{r}
extract_summary(abc)
```

```{r, custom_Ne_posterior1}
library(ggplot2)

plot_posterior(abc) +
  geom_vline(xintercept = 1000, linetype = "dashed") +
  coord_cartesian(xlim = c(100, 5000))
```

### Using external programs to compute summary statistics

Now let's move things one step forward. What if we either don't find _slendr_'s
tree-sequence interface to _tskit_ sufficient and want to compute summary
statistics from the simulated data using some external software, like PLINK or
EIGENSTRAT?

To keep things as easy as possible (and avoid dragging in multiple software
dependencies), let's say we have a program `vcfpi`, which is run in
the following way:

```
./vcfpi --vcf <path to a VCF> --tsv <path to a TSV table>
```

You can find it at `r system.file("examples/vcfpi", package = "demografr")`) and
run it yourself in the terminal.

This program takes in a VCF file and computes the nucleotide diversity across
all individuals in that VCF file. For simplicity, it produces a table of
results in exactly the same format as the `observed_diversity` data frame
above. 

In this particular example, we want to replace the computation of statistics
performed by the R function `compute_diversity` in the previous section
(which also produces a data frame) with the product of the external software
`vcfpi`.

(We provide this `vcfpi` program as an example script, but it could
of course be any other program you could imagine.)

First, the definitions of `priors` and `observed` statistics do not change:

```{r}
# a single prior parameter
priors <- list(Ne ~ runif(100, 5000))

# a single observed statistic
observed <- list(diversity = observed_diversity)
```

What does change is the simulated summary statistics definition. There are
a number of ways we could go about this (and we will show others below),
but given that our `vcfpi` is a command-line program, we will use the function


```{r}
compute_pi_vcf <- function(vcf) {
  # path to the output summary statistic file
  tsv <- tempfile()

  # path to the example command-line utility (in this example we use a built-in toy program
  # called `vcfpi`, but this could be a path to any other program of your choosing, of course)
  program <- system.file("examples/vcfpi", package = "demografr")

  # execute the program on the command line using appropriate arguments
  system2(program, args = c("--vcf", vcf, "--tsv", tsv))

  # read the computed table with computed nucleotide diversity
  # (for simplicity, our example vcfpi program creates an output TSV
  # file already in the necessary format but any required reformatting
  # changes could easily happen at this stage)
  read.table(tsv, header = TRUE)
}

functions <- list(diversity = compute_pi_vcf)
```

There's one last thing we need to do -- our summary function `compute_diversity`
does not operate on a tree-sequence file this time. Instead, `vcfpi` requires
a VCF file as its input. But, because of our simulation model `python_script`
still creates a tree-sequence file, we need to create a VCF to be able to
compute nucleotide diversity.

(Of course, we could modify the simulation model script to produce such VCF
easily, but for the sake of making this example a bit more educational, let's
make our job a little bit harder).

Whenever we need to run a simulation summary statistic on a different form of
data than a tree-sequence, we can further customize the data-generating
function(s). In the first example, we had to provide a function which creates
a `ts` object from `path` to a directory with all result files. In this
example, let's push this one step further and create a VCF file.

(Again, we could've just as easily create a VCF file from our simulation script,
but bear with me here!)

```{r}
gens <- list(
  vcf = function(path) {
    # read the simulated tree sequence from a given path and subset it
    # (same code as previously, where we worked exclusively on tree sequences)
    ts_path <- file.path(path, "result.trees")
    ts <- ts_read(ts_path) %>% ts_simplify(simplify_to = seq(0, 99))

    # export genotypes from the tree sequence to a VCF (in the same directory)
    vcf_path <- file.path(path, "genotypes.vcf.gz")
    ts_vcf(ts, vcf_path)

    # return the path to the VCF for use in computing summary statistics
    return(vcf_path)
  }
)
```

```{r}
model_run <- simulate_model(python_script, parameters = list(Ne = 123), format = "files", data = gens)
```

When we look at the result, we can see that it no longer contains a `ts`
tree-sequence object but a path to a VCF file!

```{r}
model_run
```

Let's test our summary function on that VCF:

```{r}
functions$diversity(model_run$vcf)
```

Or, for convenience, we can do this using the more general function
`summarise_data()`:

```{r}
summarise_data(model_run, functions)
```

Now let's validate our customized setup before we unleash the full scale of ABC
simulations on the problem!

```{r}
validate_abc(python_script, priors, functions, observed, data = gens, format = "files")
```

```{r, echo=FALSE, eval=TRUE}
tstart <- Sys.time()
```

```{r, eval=!on_windows && !file.exists(abc_path2) && rerun}
library(future)
plan(multisession, workers = availableCores()) # parallelize across all CPUs

data <- simulate_abc(
  model = python_script, priors, functions, observed, iterations = 10000,
  data = gens, format = "files"
)
```

```{r, echo=FALSE, eval=TRUE}
tend <- Sys.time()
tdelta <- as.numeric(difftime(tend, tstart, units = "secs"))
ncores <- future::availableCores()
```

```{r, echo=FALSE, eval=!on_windows && !file.exists(abc_path2)}
saveRDS(tdelta, get_cache("examples/custom_tdelta2.rds"))
saveRDS(ncores, get_cache("examples/custom_ncores2.rds"))
```

```{r, echo=FALSE, eval=TRUE}
tdelta <- readRDS(get_cache("examples/custom_tdelta2.rds"))
ncores <- readRDS(get_cache("examples/custom_ncores2.rds"))
```

```{r, echo=FALSE, eval=TRUE}
hours <- floor(tdelta / 3600)
minutes <- floor((tdelta - hours * 3600) / 60)
seconds <- round(tdelta - hours * 3600 - minutes * 60)
```

**The total runtime for the ABC simulations was
`r paste(hours, "hours", minutes, "minutes", seconds, "seconds")`
parallelized across `r ncores` CPUs.**

Once the simulations are finished, we can perform inference of the posterior
distribution of the single parameter of our model, $N_e$:

```{r, eval=!on_windows && !file.exists(abc_path2)}
abc <- run_abc(data, engine = "abc", tol = 0.01, method = "neuralnet")
```

```{r, echo=FALSE, eval=!on_windows && !file.exists(abc_path2)}
saveRDS(abc, abc_path2)
```

```{r, echo=FALSE, eval=file.exists(abc_path2)}
abc <- readRDS(abc_path2)
```
```{r}
extract_summary(abc)
```

```{r, custom_Ne_posterior2}
library(ggplot2)

plot_posterior(abc) +
  geom_vline(xintercept = 1000, linetype = "dashed") +
  coord_cartesian(xlim = c(100, 5000))
```

### Another means of computing simulated summary statistics

Let's look at a yet another variation of the same. What if we want to compute
a summary statistics in R on a different source of genotype data (not
a tree-sequence object in memory but also not a VCF file stored on disk)?

For instance, what if we want to compute a summary statistics on a simple
data frame of genotypes (`0` -- ancestral state, `1` -- derived state).

We can modify our pipeline in the following way.

First, we need to define a summary statistic function which will operate on
a data frame, say called `gt`.

```{r}
compute_diversity <- function(gt) {
  # generate a list of all pairs of chromosomes
  pairs <- combn(names(gt[, -1]), 2, simplify = FALSE)

  # count the number of nucleotide differences for each pair
  diffs <- sapply(pairs, function(pair) sum(gt[[pair[1]]] != gt[[pair[2]]]))
  
  # the mean is the nucleotide diversity of the sample of chromosomes
  pi <- mean(diffs) / 1e6

  data.frame(set = "pop", diversity = pi)
}

functions <- list(diversity = compute_diversity)
```

We also need a data-generating function, which will create the table of
genotypes from standard simulated output:

```{r}
gens <- list(
  gt = function(path) {
    # read the simulated tree sequence from a given path and subset it
    ts_path <- file.path(path, "result.trees")
    ts <- ts_read(ts_path) %>% ts_simplify(simplify_to = seq(0, 99))
    # suppress warnings due to multiallelic sites
    suppressWarnings(ts_genotypes(ts))
  }
)
```

And that's it! Let's check again that the whole setup works correctly.

```{r}
model_run <- simulate_model(python_script, parameters = list(Ne = 123), format = "files", data = gens)
```

When we look at the result, we can see that it no longer contains a `ts`
tree-sequence object but a simple table of genotypes created via the _slendr_
function `ts_genotypes()` (see the data generating function just above).

```{r}
model_run
```

Let's test our summary function on the output of the single testing simulation
run:

```{r}
summarise_data(model_run, functions)
```

Let's also validate our customized setup before we unleash the full scale of ABC
simulations on the problem!

```{r}
validate_abc(python_script, priors, functions, observed, data = gens, format = "files")
```

```{r, echo=FALSE, eval=TRUE}
tstart <- Sys.time()
```

```{r, eval=!on_windows && !file.exists(abc_path3) && rerun}
library(future)
plan(multisession, workers = availableCores())

data <- simulate_abc(
  model = python_script, priors, functions, observed, iterations = 10000,
  data = gens, format = "files"
)
```

```{r, echo=FALSE, eval=TRUE}
tend <- Sys.time()
tdelta <- as.numeric(difftime(tend, tstart, units = "secs"))
ncores <- future::availableCores()
```

```{r, echo=FALSE, eval=!on_windows && !file.exists(abc_path3)}
saveRDS(tdelta, get_cache("examples/custom_tdelta3.rds"))
saveRDS(ncores, get_cache("examples/custom_ncores3.rds"))
```

```{r, echo=FALSE, eval=TRUE}
tdelta <- readRDS(get_cache("examples/custom_tdelta3.rds"))
ncores <- readRDS(get_cache("examples/custom_ncores3.rds"))
```

```{r, echo=FALSE, eval=TRUE}
hours <- floor(tdelta / 3600)
minutes <- floor((tdelta - hours * 3600) / 60)
seconds <- round(tdelta - hours * 3600 - minutes * 60)
```

**The total runtime for the ABC simulations was
`r paste(hours, "hours", minutes, "minutes", seconds, "seconds")`
parallelized across `r ncores` CPUs.**

Once the simulations are finished, we can perform inference of the posterior
distribution of the single parameter of our model, $N_e$:

```{r, eval=!on_windows && !file.exists(abc_path3)}
abc <- run_abc(data, engine = "abc", tol = 0.01, method = "neuralnet")
```

```{r, echo=FALSE, eval=!on_windows && !file.exists(abc_path3)}
saveRDS(abc, abc_path3)
```

```{r, echo=FALSE, eval=file.exists(abc_path3)}
abc <- readRDS(abc_path3)
```
```{r}
extract_summary(abc)
```

```{r, custom_Ne_posterior3}
library(ggplot2)

plot_posterior(abc) +
  geom_vline(xintercept = 1000, linetype = "dashed") +
  coord_cartesian(xlim = c(100, 5000))
```


### Computing summary statistics with Python

As a last example for customization of _demografr_ inference pipeline, let's
consider the scenario in which the tree-sequence functionality available
through _slendr_ isn't sufficient, and you need to compute summary statistics
on the simulated tree-sequence using pure Python and the _tskit_ module 
directly. As with the other customization examples above, this is again very
easy to do.

First, just to change things up, let's say that we want to use a normal 
_slendr_ model function as a scaffold model for inference, simulate a
tree sequence with the standard _slendr_ / _demografr_ functionality
(i.e, not via the external Python script as in the examples above), and
only customize the summary statistic computation of nucleotide diversity.

```{r}
model <- function(Ne) {
  # this will be a single-population coalescent model, so the time of
  # the appearance of the population is arbitrary -- let's say we
  # want the model to start at 1000 generations ago, just to pick an
  # arbitrary point in time
  pop <- population("pop", N = Ne, time = 1000)
  model <- compile_model(
    pop, generation_time = 1,
    direction = "backward",
    serialize = FALSE
  )
  schedule <- schedule_sampling(model, times = 0, list(pop, 50))
  return(list(model, schedule))
}
```

We will use the same prior and observed statistic as above:

```{r}
observed_diversity <- readRDS(get_cache("examples/custom_diversity.rds"))

priors <- list(Ne ~ runif(100, 5000))

observed <- list(diversity = observed_diversity)
```

```{r, echo=FALSE}
# Equivalent computations to the Python code right below

# compute_diversity <- function(ts) {
#   ts_diversity(ts, list(pop = seq(0, 99)), mode = "site")
# }

# compute_divergence <- function(ts) {
#   pairs <- combn(0:99, 2, simplify = FALSE)
#   diffs <- sapply(pairs, function(pair) ts_divergence(ts, pair, mode = "site")$divergence)
#   mean(diffs)
# }
```

```{r}
compute_diversity <- function(ts) {
  reticulate::py_run_string(r"(
def compute_diversity(ts):
    diffs = []

    for i in range(ts.num_samples - 1):
        for j in range(i + 1, ts.num_samples):
            diffs.append(ts.divergence(sample_sets=[[i], [j]], mode="site"))

    pi = sum(diffs) / len(diffs)

    return pd.DataFrame({"set": ["pop"], "diversity": [pi]})
)"
  )
  reticulate::py$compute_diversity(ts)
}

functions <- list(diversity = compute_diversity)
```

```{r}
model_run <- simulate_model(model, parameters = priors, sequence_length = 1e6, recombination_rate = 1e-8)
```

When we look at the result, we can see that it no longer contains a `ts`
tree-sequence object but a simple table of genotypes created via the _slendr_
function `ts_genotypes()` (see the data generating function just above).

```{r}
model_run
```

Let's test our summary function on the output of the single testing simulation
run:

```{r}
summarise_data(model_run, functions)
```

Let's also validate our customized setup before we unleash the full scale of ABC
simulations on the problem!

```{r}
validate_abc(model, priors, functions, observed,
             sequence_length = 1e6, recombination_rate = 0)
```

```{r, echo=FALSE, eval=TRUE}
tstart <- Sys.time()
```

```{r, eval=!on_windows && !file.exists(abc_path4) && rerun}
library(future)
plan(multisession, workers = availableCores())

data <- simulate_abc(
  model, priors, functions, observed, iterations = 10000,
  sequence_length = 1e6, recombination_rate = 1e-8, mutation_rate = 1e-8
)
```

```{r, echo=FALSE, eval=TRUE}
tend <- Sys.time()
tdelta <- as.numeric(difftime(tend, tstart, units = "secs"))
ncores <- future::availableCores()
```

```{r, echo=FALSE, eval=!on_windows && !file.exists(abc_path4)}
saveRDS(tdelta, get_cache("examples/custom_tdelta4.rds"))
saveRDS(ncores, get_cache("examples/custom_ncores4.rds"))
```

```{r, echo=FALSE, eval=TRUE}
tdelta <- readRDS(get_cache("examples/custom_tdelta4.rds"))
ncores <- readRDS(get_cache("examples/custom_ncores4.rds"))
```

```{r, echo=FALSE, eval=TRUE}
hours <- floor(tdelta / 3600)
minutes <- floor((tdelta - hours * 3600) / 60)
seconds <- round(tdelta - hours * 3600 - minutes * 60)
```

**The total runtime for the ABC simulations was
`r paste(hours, "hours", minutes, "minutes", seconds, "seconds")`
parallelized across `r ncores` CPUs.**

Once the simulations are finished, we can perform inference of the posterior
distribution of the single parameter of our model, $N_e$:

```{r, eval=!on_windows && !file.exists(abc_path4)}
abc <- run_abc(data, engine = "abc", tol = 0.01, method = "neuralnet")
```

```{r, echo=FALSE, eval=!on_windows && !file.exists(abc_path4)}
saveRDS(abc, abc_path4)
```

```{r, echo=FALSE, eval=file.exists(abc_path4)}
abc <- readRDS(abc_path4)
```

```{r}
extract_summary(abc)
```

```{r, custom_Ne_posterior4}
library(ggplot2)

plot_posterior(abc) +
  geom_vline(xintercept = 1000, linetype = "dashed") +
  coord_cartesian(xlim = c(100, 5000))
```
