---
title: "Diagnostics, model selection, troubleshooting"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Diagnostics, model selection, troubleshooting}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
slendr_present <- slendr::check_dependencies(python = TRUE, slim = TRUE, quit = FALSE)

knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#>",
  fig.align = "center",
  fig.width = 8,
  fig.height = 5,
  dpi = 80,
  eval = slendr_present
)

devtools::install(upgrade = FALSE)
devtools::load_all()

dataX_path <- here::here("inst/examples/downstream_dataX.rds")
dataY_path <- here::here("inst/examples/downstream_dataY.rds")
dataZ_path <- here::here("inst/examples/downstream_dataZ.rds")
data_exists <- all(c(file.exists(dataX_path), file.exists(dataY_path), file.exists(dataZ_path)))

abcX_path <- here::here("inst/examples/downstream_abcX.rds")
abcY_path <- here::here("inst/examples/downstream_abcY.rds")
abcZ_path <- here::here("inst/examples/downstream_abcZ.rds")
abc_exists <- all(c(file.exists(abcX_path), file.exists(abcY_path), file.exists(abcZ_path)))

predX_path <- here::here("inst/examples/downstream_predX.rds")
predY_path <- here::here("inst/examples/downstream_predY.rds")
predZ_path <- here::here("inst/examples/downstream_predZ.rds")
pred_exists <- all(c(file.exists(predX_path), file.exists(predY_path), file.exists(predZ_path)))
```

```{r, collapse=TRUE}
library(demografr)
library(slendr)
init_env()

library(future)
plan(multisession, workers = availableCores())

SEED <- 42
set.seed(SEED)
```

⚠️⚠️⚠️

**Note:** The _demografr_ R package is still under active development. As a result, its documentation is in a draft stage at best. Typos, inconsistencies, and other issues are unfortunately expected.

⚠️⚠️⚠️

Let's return to our [first](vignette-01-basics.html) example. However, this time, imagine that we don't really know which of the three following phylogenetic relationships is the one that captures the features of our data best, perhaps with different sources of evidence being consistent with each one of them so we want to consider all three in our modeling (as always, this is purely a toy example). These models differ both in terms of tree topology of the population relationships as well as in the gene flow, and we want to perform *model selection*.

```{r ape_tree_modelX, echo=FALSE, fig.width=5, fig.height=5}
orig_par <- par(no.readonly = TRUE)

par(mar = c(0, 0, 2, 0), mfrow = c(3, 1))

tree <- ape::read.tree(text="(popA,(popB,(popC,popD)));")
plot(tree, main = "model X")
arrows(2.5, 2, 2.5, 3, col="blue")

tree <- ape::read.tree(text="(((popA,popB),popC),popD);")
plot(tree, main = "model Y")
arrows(2.5, 1, 2.5, 2, col="blue")

tree <- ape::read.tree(text="((popA,popB),(popC,popD));")
plot(tree, main = "model Z")
arrows(2.5, 3, 2.5, 4, col="blue")

par(orig_par)
```

For completeness, here is again our observed data which was, in reality, generated by a hidden evolutionary process we're trying to understand by running ABC:

1. Nucleotide diversity in each population:

```{r}
observed_diversity <- read.table(system.file("examples/basics_diversity.tsv", package = "demografr"), header = TRUE)

observed_diversity
```

2. Pairwise divergence d_X_Y between populations X and Y:

```{r}
observed_divergence <- read.table(system.file("examples/basics_divergence.tsv", package = "demografr"), header = TRUE)

observed_divergence
```

3. Value of the following $f_4$-statistic:

```{r}
observed_f4  <- read.table(system.file("examples/basics_f4.tsv", package = "demografr"), header = TRUE)

observed_f4
```

We will again bind them into a list:

```{r}
observed <- list(diversity  = observed_diversity, divergence = observed_divergence, f4 = observed_f4)
```


### Three competing models

First, in order to perform model selection, we need to specify models themselves. We do this by defining three separate _slendr_ functions, each of them encoding the three phylogenetic relationships from the diagrams above. Note that we're not trying to infer the time of gene flow, for simplicity, but we do fix the gene flow event to different times (again, reflected in the alternative model sketches above):

```{r}
modelX <- function(Ne_A, Ne_B, Ne_C, Ne_D, T_1, T_2, T_3, gf) {
  popA <- population("popA", time = 1,   N = Ne_A)
  popB <- population("popB", time = T_1, N = Ne_B, parent = popA)
  popC <- population("popC", time = T_2, N = Ne_C, parent = popB)
  popD <- population("popD", time = T_3, N = Ne_D, parent = popC)

  gf <- gene_flow(from = popB, to = popC, start = 9000, end = 9301, rate = gf)

  model <- compile_model(
    populations = list(popA, popB, popC, popD), gene_flow = gf,
    generation_time = 1, simulation_length = 10000,
    direction = "forward"
  )

  samples <- schedule_sampling(
    model, times = 10000,
    list(popA, 50), list(popB, 50), list(popC, 50), list(popD, 50),
    strict = TRUE
  )

  return(list(model, samples))
}

modelY <- function(Ne_A, Ne_B, Ne_C, Ne_D, T_1, T_2, T_3, gf) {
  popA <- population("popA", time = 1,   N = Ne_A)
  popB <- population("popB", time = T_1, N = Ne_B, parent = popA)
  popC <- population("popC", time = T_2, N = Ne_C, parent = popA)
  popD <- population("popD", time = T_3, N = Ne_D, parent = popA)

  gf <- gene_flow(from = popA, to = popB, start = 9000, end = 9301, rate = gf)

  model <- compile_model(
    populations = list(popA, popB, popC, popD), gene_flow = gf,
    generation_time = 1, simulation_length = 10000,
    direction = "forward"
  )

  samples <- schedule_sampling(
    model, times = 10000,
    list(popA, 50), list(popB, 50), list(popC, 50), list(popD, 50),
    strict = TRUE
  )

  return(list(model, samples))
}

modelZ <- function(Ne_A, Ne_B, Ne_C, Ne_D, T_1, T_2, T_3, gf) {
  popA <- population("popA", time = 1,   N = Ne_A)
  popB <- population("popB", time = T_1, N = Ne_B, parent = popA)
  popC <- population("popC", time = T_2, N = Ne_C, parent = popA)
  popD <- population("popD", time = T_3, N = Ne_D, parent = popC)

  gf <- gene_flow(from = popC, to = popD, start = 9000, end = 9301, rate = gf)

  model <- compile_model(
    populations = list(popA, popB, popC, popD), gene_flow = gf,
    generation_time = 1, simulation_length = 10000,
    direction = "forward"
  )

  samples <- schedule_sampling(
    model, times = 10000,
    list(popA, 50), list(popB, 50), list(popC, 50), list(popD, 50),
    strict = TRUE
  )

  return(list(model, samples))
}

```

```{r, echo=FALSE, eval=FALSE}
modelX(1, 1, 1, 1, 2000, 6000, 8000, 0.5) %>% .[[1]] %>% plot_model(order = c("popA", "popB", "popC", "popD"), file = "modelX.pdf")
modelY(1, 1, 1, 1, 8000, 6000, 2000, 0.5) %>% .[[1]] %>% plot_model(order = c("popA", "popB", "popC", "popD"), file = "modelY.pdf")
modelZ(1, 1, 1, 1, 8000, 1000, 8000, 0.5) %>% .[[1]] %>% plot_model(order = c("popA", "popB", "popC", "popD"), file = "modelZ.pdf")
```

Now, let's [specify priors](vignette-02-priors.html) using _demografr_'s [templating syntax](vignette-02-priors.html#prior-parameter-templates). This saves us a bit of typing, making the prior definition code a bit more consise and easier to read:

```{r}
priors <- list(
  Ne... ~ runif(100, 10000),

  T_1   ~ runif(1,    4000),
  T_2   ~ runif(3000, 9000),
  T_3   ~ runif(5000, 10000),

  gf    ~ runif(0, 1)
)
```

Let's also put together a list of tree-sequence summary functions and observed summary statistics:

```{r}
compute_diversity <- function(ts) {
  samples <- ts_names(ts, split = "pop")
  ts_diversity(ts, sample_sets = samples)
}

compute_divergence <- function(ts) {
  samples <- ts_names(ts, split = "pop")
  ts_divergence(ts, sample_sets = samples)
}

compute_f4 <- function(ts) {
  samples <- ts_names(ts, split = "pop")
  A <- samples["popA"]; B <- samples["popB"]
  C <- samples["popC"]; D <- samples["popD"]
  ts_f4(ts, A, B, C, D)
}

functions <- list(
  diversity = compute_diversity,
  divergence = compute_divergence,
  f4 = compute_f4
)
```

Let's validate the ABC setup of all three models -- this is an important check that the _slendr_ model functions are defined correctly:

```{r}
validate_abc(modelX, priors, functions, observed, quiet = TRUE)
validate_abc(modelY, priors, functions, observed, quiet = TRUE)
validate_abc(modelZ, priors, functions, observed, quiet = TRUE)
```

```{r, echo=FALSE, eval=FALSE}
tsX <- simulate_model(modelX, priors, sequence_length = 1e6, recombination_rate = 0)
tsY <- simulate_model(modelY, priors, sequence_length = 1e6, recombination_rate = 0)
tsZ <- simulate_model(modelZ, priors, sequence_length = 1e6, recombination_rate = 0)
```

With that out of the way, we can proceed with generating simulated data for inference using all three models. What we'll do is perform three runs and save them into appropriately named variables `dataX`, `dataY`, and `dataZ`:

```{r, echo=FALSE}
tstart <- Sys.time()
```

```{r, eval=!data_exists}
dataX <- simulate_abc(modelX, priors, functions, observed, iterations = 10000,
                      sequence_length = 10e6, recombination_rate = 1e-8, mutation_rate = 1e-8)
```

```{r, eval=!data_exists}
dataY <- simulate_abc(modelY, priors, functions, observed, iterations = 10000,
                      sequence_length = 10e6, recombination_rate = 1e-8, mutation_rate = 1e-8)
```

```{r, eval=!data_exists}
dataZ <- simulate_abc(modelZ, priors, functions, observed, iterations = 10000,
                      sequence_length = 10e6, recombination_rate = 1e-8, mutation_rate = 1e-8)
```

```{r, eval=!data_exists, echo=FALSE}
tend <- Sys.time()
tdelta <- as.numeric(difftime(tend, tstart, units = "secs"))
ncores <- future::availableCores()
```

```{r, eval=!data_exists, echo=FALSE}
saveRDS(tdelta, here::here("inst/examples/downstream_tdelta.rds"))
saveRDS(ncores, here::here("inst/examples/downstream_ncores.rds"))
saveRDS(dataX, dataX_path)
saveRDS(dataY, dataY_path)
saveRDS(dataZ, dataZ_path)
```

```{r, eval=data_exists, echo=FALSE}
tdelta <- readRDS(here::here("inst/examples/downstream_tdelta.rds"))
ncores <- readRDS(here::here("inst/examples/downstream_ncores.rds"))
dataX <- readRDS(dataX_path)
dataY <- readRDS(dataY_path)
dataZ <- readRDS(dataZ_path)
```

```{r, echo=FALSE, eval=TRUE}
hours <- floor(tdelta / 3600)
minutes <- floor((tdelta - hours * 3600) / 60)
seconds <- round(tdelta - hours * 3600 - minutes * 60)
```


**The total runtime for the ABC simulations was `r paste(hours, "hours", minutes, "minutes", seconds, "seconds")` parallelized across `r ncores` CPUs.**

```{r, eval=!abc_exists, results="hide"}
abcX <- run_abc(dataX, engine = "abc", tol = 0.01, method = "neuralnet")
abcY <- run_abc(dataY, engine = "abc", tol = 0.01, method = "neuralnet")
abcZ <- run_abc(dataZ, engine = "abc", tol = 0.01, method = "neuralnet")
```

```{r, eval=!abc_exists, echo=FALSE}
saveRDS(abcX, abcX_path)
saveRDS(abcY, abcY_path)
saveRDS(abcZ, abcZ_path)
```

```{r, eval=abc_exists, echo=FALSE}
abcX <- readRDS(abcX_path)
abcY <- readRDS(abcY_path)
abcZ <- readRDS(abcZ_path)
```

## Cross-validation

Before doing model selection, it's important to perform cross-validation to answer the question whether our ABC setup can even distinguish between the competing models. Without having the power to do this, trying to select the best model wouldn't make much sense.

This can be done using _demografr_'s `cross_validate()` function which is built around _abc_'s own function `cv4postpr()`. We will not go into too much detail, as this function simply calls `cv4postpr()` under the hood, passing to it all specified function arguments on behalf of a user to avoid unnecessary manual data munging (creating the `index` vector, merging the summary statistics into `sumstat` parameter, etc.). For more details, read section "Model selection" in the [vignette](https://cran.r-project.org/package=abc/vignettes/abcvignette.pdf) of the _abc_ R package.

The one difference between the two functions is that `cross_validate()` removes the need to prepare character indices and bind together summary statistic matrices from different models&mdash;given that _demografr_'s ABC output objects track all this information along in their internals, this is redundant, and you can perform cross-validation of different ABC models simply by calling this:

```{r, echo=FALSE}
set.seed(42)
```

```{r, results="hide", warning=FALSE}
models <- list(abcX, abcY, abcZ)

cv_selection <- cross_validate(models, nval = 100, tol = 0.01, method = "neuralnet")
```

If we print out the result, we get a quick summary of the resulting confusion matrices and other diagnostic information:

```{r}
cv_selection
```

Similarly, you can use the `plot()` function to visualize the result. This function, yet again, internally calls _abc_'s own plotting method internall, with a bonus option to save a figure to a PDF right from the `plot()` call (useful when working on a remote server):

```{r plot_cv}
plot(cv_selection)
```

Because we have three models, each of the three barplots shows how often were summary statistics sampled from each model classified as likely coming from one of the three models. In other words, with absolutely perfect classification, each barplot would show just one of the three colors. If a barplot (results for one model) shows multiple colors, this means that some fraction of simulated statistics from that model was incorrectly classified as another model. Again, for more detail on interpretation, caveats, and best practices, please consult the _abc_ R package [vignette](https://cran.r-project.org/package=abc/vignettes/abcvignette.pdf) and a relevant statistical textbook.

The confusion matrices and the visualization all suggest that ABC can distinguish between the three models very well. For instance, _modelX_ has been classified correctly in `r cv_selection$estim[[1]] %>% { .[names(.) == "modelX"] } %>% { names(.) == . } %>% sum` simulations out of the total of 100 cross-validation simulations, with an overall misclassification rate for all models of only `{r sprintf("%.1f", cv_selection$estim[[1]] %>%  { 1 - mean(names(.) == .) } * 100)}`%.

## Model selection

Armed with confidence in the ability of ABC to correctly identify the correct model based on simulated data, we can proceed to selection of the best model for our empirical data set. This can be done with the function `select_model()` which is _demografr_'s convenience wrapper around _abc_'s own function `postpr`:

```{r, results="hide"}
models <- list(abcX, abcY, abcZ)

modsel <- select_model(models, tol = 0.03, method = "neuralnet")
```

We can make a decision on the model selection by inspecting the `summary()` of the produced result:

```{r}
modsel
```

As we can see, `modelX` shows the highest rate of acceptance among all simulations. Specifically, we see that the proportion of acceptance is `r sprintf("%.1f", max(quiet(summary(modsel))$rejection$Prob) * 100)`%.

Similarly, looking at the Bayes factors, we see that the "modelX" is `r sprintf("%.1f", quiet(summary(modsel))$rejection$Bayes[1, 2])` times more likely than "modelY", and `r sprintf("%.1f", quiet(summary(modsel))$rejection$Bayes[1, 3])` more likely than "modelZ".

When the posterior probabilities are computed using a more elaborate neural network method (again, see more details in the _abc_ [vignette](https://cran.r-project.org/package=abc/vignettes/abcvignette.pdf)), we find that the probability of "modelX" is even more overwhelming. In fact, the analysis shows that it has a 100% probabiliy of being the correct model to explain our data.

**But how accurate are these conclusions?** Well, if we take a peek at the _slendr_ model which was [internally used](https://github.com/bodkan/demografr/blob/main/vignettes/vignette-01-basics.Rmd#L36-L4) to generate the "observed" summary statistics, we see that the data was indeed simulated by code nearly identical to the one shown above as `modelX`. When plotted, the true model looks like this:

```{r, true_model}
popA <- population("popA", time = 1, N = 2000)
popB <- population("popB", time = 2000, N = 800, parent = popA)
popC <- population("popC", time = 6000, N = 9000, parent = popB)
popD <- population("popD", time = 8000, N = 4000, parent = popC)

gf <- gene_flow(from = popB, to = popC, start = 9000, end = 9301, rate = 0.1)

example_model <- compile_model(
  populations = list(popA, popB, popC, popD), gene_flow = gf,
  generation_time = 1,
  simulation_length = 10000
)

plot_model(example_model, proportions = TRUE)
```

It appears that we can be quite confident in which of the three models is most compatible with the data. However, before we move on to inferring parameters of the model, we should check whether the best selected model can indeed capture the most important features of the data, which is in case of ABC represented by our summary statistics.

In order to do this, _demografr_ provides a function call `predict()` which performs posterior predictive check. Below, we we also look at how to use functions `gfit()` and `gfitpca()` implemented by the _abc_ R package itself.

Let's start with the posterior predictive checks.

## Posterior predictive checks

Link: [](http://www.stat.columbia.edu/~gelman/research/published/A6n41.pdf)

It is one thing to 
Simply speaking, when fitting a model and estimating its parameters using ABC, we rely on a set of summary statistics, which we assume capture the most important features of the true model represented by the data we gathered. Performing posterior predictive checks involves estimating the posterior distributions of the parameters of interest from the data (i.e. doing ABC inference) -- something we've done above on our `modelX`, `modelY`, and `modelZ` -- and then sampling parameters from these distributions and generating distributions of simulated summary statistics from those estimated models. Then, we plot the distributions of each of our _simulated_ summary statistics together with a vertical line representing the values of the _observed_ statistics. Naturally, only if the model can generate statistics compatible with the values observed in real data can we regard the model as sensible and usable for inference.

```{r, eval=!pred_exists}
plan(multisession, workers = availableCores())
predX <- predict(abcX, samples = 1000, posterior = "unadj")
predY <- predict(abcY, samples = 1000, posterior = "unadj")
predZ <- predict(abcZ, samples = 1000, posterior = "unadj")
```

```{r, echo=FALSE, eval=!pred_exists}
saveRDS(predX, predX_path)
saveRDS(predY, predY_path)
saveRDS(predZ, predZ_path)
```

```{r, echo=FALSE, eval=pred_exists}
predX <- readRDS(predX_path)
predY <- readRDS(predY_path)
predZ <- readRDS(predZ_path)
```

Looking at the posterior predictive plots from the _model X_, we can see that the observed statistics (vertical dashed lines) fall very well within the distributions of statistics simulated from the posterior parameters of the model. This suggests that are model (simplified that it must be given the more complex reality), captures the empoirical statistics quite well:

```{r}
plot_prediction(predX)
```

On the other hand, looking at the _model Z_, for instance, we see that although the $f_4$ gene-flow detection statistic fits rather nicely, the pairwise divergences between populations are completely from the statistics simulated from posterior distributions. This strongly suggests that the model wasn't able to capture the features of the data.

```{r}
plot_prediction(predY)
```

We can compare the divergences between models directly to see which model gives the most reasonable posterior predictive check results more clearly:

```{r}
cowplot::plot_grid(
  plot_prediction(predX, "divergence"),
  plot_prediction(predY, "divergence"),
  plot_prediction(predZ, "divergence"),
  nrow = 1
)
```

We can see that posterior predictive checks for nucleotide diversity are a little less clear as there are no extremely dramatic outliers as it's the case for divergence above:

```{r}
cowplot::plot_grid(
  plot_prediction(predX, "diversity"),
  plot_prediction(predY, "diversity"),
  plot_prediction(predZ, "diversity"),
  nrow = 1
)
```

```{r}
cowplot::plot_grid(
  plot_prediction(predX, "f4"),
  plot_prediction(predY, "f4"),
  plot_prediction(predZ, "f4"),
  nrow = 1
)
```

If more customization or a more detailed analyses is needed, you can also extract data from the posterior simulations themselves:

```{r}
extract_prediction(predX, "diversity")
```

However, note that `extract_prediction` is a convenience function which only unnests, reformats, and renames the list-columns with summary statistics stored in the `predictions` data frame above. You can do all the processing based on the `predictions` object above.



## Additional diagnostics and backwards compatibility with the _abc_ package

As mentioned above, the functionality behind `cross_validate()` is implemented by reformatting the results of _demografr_ simulation and inference functionality in a form necessary to run the functions of the _abc_ package `cv4abc()` and `cv4postpr()` behind the scenes -- a task that normally requires a significant amount of potentially error-prone bookkeeping on part of the user.

In addition to _demografr_'s own function `predict()` which implements posterior predictive checks, the _abc_ package provides additional functionality for evaluating the quality of the fit of an ABC model to the data, namely `gfit()` and `gfitpca()`. In this section we demonstrate how this functionality (and potentially other future _abc_ functions) can be applied on _demografr_ data with very little additional work.

First, _demografr_ provides a function `unpack()` which -- when applied on an object generated either by `simulate_abc()` or `run_abc()` -- _unpacks_ the object encapsulating all information describing the simulation and inference which produced it, into individual components with names matching the "low-level" function arguments of all relevant functions of the _anc_ package. This includes `gfit()` and `gfitpca()` mentioned above, but also functions `cv4abc()` and `cv4postpr()`. Here is how `unpack()` works.

### "Unpacking" a _demografr_ object to low-level components used by the _abc_ package

```{r}
partsX <- unpack(abcX)

class(partsX)
names(partsX)
```

As we can see, the `unpack()` function produced a list object with several elements: `r names(partsX)`. It's not a coincidence that the names of these list elements correspond to function arguments of various diagnostic functions of the _abc_ package. In this way, _demografr_ pre-generates the information required by those functions (which normally need to be prepared manually), which can then be run without any additional work whatsoever. For instance, we could perform the cross-validation of the model selection using _abc_'s own functionality like this.

First, because model-selection requires, well, multiple models to select from, we will unpack information from all the three models:

```{r}
parts <- unpack(list(abcX, abcY, abcZ))

class(parts)
names(parts)
```

Then, we simply run `cv4postpr()` as we normally would, using the information pre-generated by `unpack()`:

```{r}
cv4postpr_res <- cv4postpr(index = parts$index, sumstat = parts$sumstat, nval = 10, tols = 0.01, method = "neuralnet")
```

Unlike the result of `cross_validate()` above, typing out the object `cv4postpr_res` will not help us, but we can get the overview of the results by applying the function `summary()`:

```{r}
summary(cv4postpr_res)
```

As you can see, we can replicate the result of `cross_validate()` for model selection by combining the functions `unpack()`, `cv4postpr()` and `summary()`. Of course, using `cross_validate()` allows us to do this all in one go in a straightforward way, but the above hopefully demonstrates that there's no magic behind _demografr_'s convenient functionality. It just streamlines the process of ABC diagnostics and avoids unnecessary (and potentially error-prone) data processing.

On a similar note, we can also run _abc_ functions `gfit()` and `gfitpca()` (as described in [this vignette](https://cran.r-project.org/web/packages/abc/vignettes/abcvignette.pdf)) in exactly the same way. We will focus just on the code, and leave the interpretation to the discussion in the vignette:

First, we will unpack the results of the selected best-fitting model from the appropriate `abc` object obtained by the _demografr_ function `run_abc()` above:

```{r}
partsX <- unpack(abcX)

class(partsX)
names(partsX)
```

Then we use the elements of the parts list that we just created as inputs for `gfit()`:

```{r}
gfitX <- gfit(target = partsX$target, sumstat = partsX$sumstat, statistic = mean, nb.replicate = 100)

plot(gfitX, main="Histogram under H0 (model X)")

summary(gfitX)
```

```{r}
partsY <- unpack(abcY)

gfitY <- gfit(target = partsY$target, sumstat = partsY$sumstat, statistic = mean, nb.replicate = 100)

plot(gfitY, main="Histogram under H0 (model Y)")

summary(gfitY)
```

```{r}
partsZ <- unpack(abcZ)

gfitZ <- gfit(target = partsZ$target, sumstat = partsZ$sumstat, statistic = mean, nb.replicate = 100)

plot(gfitZ, main="Histogram under H0 (model Z)")

summary(gfitZ)
```

For completeness, here is how we can run `gfitpca()` on _demografr_ results:

```{r}
parts <- unpack(list(abcX, abcY, abcZ))

# this needs to be loaded because of a bug in abc dependency handling
library(locfit)
gfitpca(target = parts$target, sumstat = parts$sumstat, index = parts$index, cprob = 0.01,
        xlim = c(-7, 7), ylim = c(-6, 3))
```

Based on the PCA, it appears that although model X and model Y are able to generate summary statistics (the areas captured by the colored "envelopes" in 2D space) compatible with the observed values (the cross), modelZ clearly cannot.


## ABC cross-validation

Now we are finally at a point at which we can infer the parameters of our model against the data. However, before we do so, we should first check if our ABC setup can even estimate the model parameters at all. As we have seen just above, the _abc_ package provides a ABC cross-validation function `cv4abc()` for which _demografr_ implements a convenient interface in the form of the method `cross_validate()`. Above we have seen how to use this method to perform model selection, if given a list of multiple ABC objects as input. Instead, if given a single _demografr_ ABC object (generated by the function `run_abc()`), it performs ABC cross-validation which we can use to gauge the accuracy of ABC and the sensitivity of the parameter estimates to the tolerance rate.

```{r}
cv_abc <- cross_validate(abcX, nval = 10, tols = c(0.005, 0.01, 0.05), method = "neuralnet")
cv_abc_loclinear <- cross_validate(abcX, nval = 10, tols = c(0.005, 0.01, 0.05), method = "loclinear")
cv_abc_rejection <- cross_validate(abcX, nval = 10, tols = c(0.005, 0.01, 0.05), method = "rejection")
```

```{r}
cv_abc
```

```{r}
plot(cv_abc)
```


## Parameter inference

Now we can finally estimate the model parameters. Because we simulated the data from a (hidden) true, known model, we will also visualize the true parameter values as vertical dashed lines:

```{r}
abcX
```

```{r}
extract_summary(abcX)
```

```{r}
library(ggplot2)
```

```{r, posterior_Tsplit, warning=FALSE, fig.width=8, fig.height=5}
plot_posterior(abcX, param = "T") +
  geom_vline(xintercept = c(2000, 6000, 8000), linetype = "dashed")
```

```{r, posterior_Ne, warning=FALSE, fig.width=8, fig.height=5}
plot_posterior(abcX, param = "Ne") +
    geom_vline(xintercept = c(2000, 800, 9000, 4000), linetype = "dashed")
```

```{r, posterior_gf, warning=FALSE, fig.width=8, fig.height=5}
plot_posterior(abcX, param = "gf") +
  geom_vline(xintercept = 0.1, linetype = "dashed") +
  coord_cartesian(xlim = c(0, 1))
```

Not bad, considering how few simulations we've thrown at the inference in order to save computational resources!

